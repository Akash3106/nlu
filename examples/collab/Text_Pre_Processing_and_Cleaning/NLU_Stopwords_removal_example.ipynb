{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_Stopwords_removal_example.ipynb","provenance":[{"file_id":"1pgqoRJ6yGWbTLWdLnRvwG5DLSU3rxuMq","timestamp":1599401652794},{"file_id":"1JrlfuV2jNGTdOXvaWIoHTSf6BscDMkN7","timestamp":1599401257319},{"file_id":"1svpqtC3cY6JnRGeJngIPl2raqxdowpyi","timestamp":1599400881246},{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rBXrqlGEYA8G"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/collab/Text_Pre_Processing_and_Cleaning/NLU_Stopwords_removal_example.ipynb)\n","\n","# Stopwords removal with NLU \n","\n","Stopwords refer to the most common words in a language.       \n","\n","I. e.  'the, is, at, which,on' are stopwords which will be removed.\n","\n","\n","# 1. Install Java and NLU"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ"},"source":["\n","import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! pip install nlu > /dev/null    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_CL8HZ8Ydry"},"source":["## 2. Load Model and remove stopwords from sample string\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx","executionInfo":{"status":"ok","timestamp":1603729294747,"user_tz":-300,"elapsed":101716,"user":{"displayName":"Gammer Otaku","photoUrl":"","userId":"18042713576744284398"}},"outputId":"ea0620cd-246b-4421-b048-a31962808cb9","colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["import nlu\n","pipe = nlu.load('stopwords')\n","pipe.predict('He was suprised by the diversity of NLU')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["stopwords_en download started this may take some time.\n","Approximate size to download 2.9 KB\n","[OK!]\n","No accepted Data type or usable columns found or applying the NLU models failed. \n","Make sure that the first column you pass to .predict() is the one that nlu should predict on OR rename the column you want to predict on to 'text'  \n","If you are on Google Collab, click on Run time and try factory reset Runtime run the setup script again, you might have used too much memory\n","On Kaggle try to reset restart session and run the setup script again, you might have used too much memory\n","Full Stacktrace was (<class 'pyspark.sql.utils.AnalysisException'>, AnalysisException(\"cannot resolve '`sub_token.result`' given input columns: [token, text, document, cleanTokens, origin_index, sentence];;\\n'Project [text#68, origin_index#69L, document#73, sentence#78, token#84, cleanTokens#91, arrays_zip('sub_token.result, cleanTokens#91.result) AS tmp#112]\\n+- Project [text#68, origin_index#69L, document#73, sentence#78, token#84, cleanTokens#91]\\n   +- Project [text#68, origin_index#69L, document#73, sentence#78, token#84, UDF(array(token#84)) AS cleanTokens#91]\\n      +- Project [text#68, origin_index#69L, document#73, sentence#78, UDF(array(document#73)) AS token#84]\\n         +- Project [text#68, origin_index#69L, document#73, UDF(array(document#73)) AS sentence#78]\\n            +- Project [text#68, origin_index#69L, UDF(text#68) AS document#73]\\n               +- LogicalRDD [text#68, origin_index#69L], false\\n\", 'org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\\n\\t at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:297)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:356)\\n\\t at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\t at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\t at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\t at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\t at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\\n\\t at scala.collection.AbstractTraversable.map(Traversable.scala:104)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:356)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\\n\\t at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\\n\\t at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\t at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\t at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\t at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\t at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\\n\\t at scala.collection.AbstractTraversable.map(Traversable.scala:104)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\\n\\t at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\\n\\t at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\\n\\t at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\\n\\t at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\\n\\t at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\\n\\t at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\\n\\t at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\\n\\t at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\\n\\t at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\\n\\t at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\\n\\t at org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\\n\\t at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2258)\\n\\t at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2225)\\n\\t at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\t at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\t at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\t at java.lang.reflect.Method.invoke(Method.java:498)\\n\\t at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\t at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\t at py4j.Gateway.invoke(Gateway.java:282)\\n\\t at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\t at py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\t at py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\t at java.lang.Thread.run(Thread.java:748)'), <traceback object at 0x7fb7bd122048>)\n","Additional info:\n","<class 'pyspark.sql.utils.AnalysisException'> pipeline.py 1087\n","Stuck? Contact us on Slack! https://join.slack.com/t/spark-nlp/shared_invite/zt-7rd4kw03-7F44zohcrUo0RULCd8rYrw\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"av7EiK4adb24"},"source":["# 4. Checkout the stopword models NLU has to offer for other languages than English!"]},{"cell_type":"code","metadata":{"id":"hZ8xLHY7dgJ8","executionInfo":{"status":"ok","timestamp":1603729294751,"user_tz":-300,"elapsed":101715,"user":{"displayName":"Gammer Otaku","photoUrl":"","userId":"18042713576744284398"}},"outputId":"ea7a8d6b-a5ab-4201-b6e0-6ea5f3b526f1","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["nlu.print_all_model_kinds_for_action('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["For language <en> NLU provides the following Models : \n","nlu.load('en.stopwords') returns Spark NLP model stopwords_en\n","For language <fr> NLU provides the following Models : \n","nlu.load('fr.stopwords') returns Spark NLP model stopwords_fr\n","For language <de> NLU provides the following Models : \n","nlu.load('de.stopwords') returns Spark NLP model stopwords_de\n","For language <it> NLU provides the following Models : \n","nlu.load('it.stopwords') returns Spark NLP model stopwords_it\n","For language <pl> NLU provides the following Models : \n","nlu.load('pl.stopwords') returns Spark NLP model stopwords_pl\n","For language <pt> NLU provides the following Models : \n","nlu.load('pt.stopwords') returns Spark NLP model stopwords_pt\n","For language <ru> NLU provides the following Models : \n","nlu.load('ru.stopwords') returns Spark NLP model stopwords_ru\n","For language <af> NLU provides the following Models : \n","nlu.load('af.stopwords') returns Spark NLP model stopwords_af\n","For language <hy> NLU provides the following Models : \n","nlu.load('hy.stopwords') returns Spark NLP model stopwords_hy\n","For language <eu> NLU provides the following Models : \n","nlu.load('eu.stopwords') returns Spark NLP model stopwords_eu\n","For language <bn> NLU provides the following Models : \n","nlu.load('bn.stopwords') returns Spark NLP model stopwords_bn\n","For language <br> NLU provides the following Models : \n","nlu.load('br.stopwords') returns Spark NLP model stopwords_br\n","For language <bg> NLU provides the following Models : \n","nlu.load('bg.stopwords') returns Spark NLP model stopwords_bg\n","For language <ca> NLU provides the following Models : \n","nlu.load('ca.stopwords') returns Spark NLP model stopwords_ca\n","For language <cs> NLU provides the following Models : \n","nlu.load('cs.stopwords') returns Spark NLP model stopwords_cs\n","For language <eo> NLU provides the following Models : \n","nlu.load('eo.stopwords') returns Spark NLP model stopwords_eo\n","For language <fi> NLU provides the following Models : \n","nlu.load('fi.stopwords') returns Spark NLP model stopwords_fi\n","For language <gl> NLU provides the following Models : \n","nlu.load('gl.stopwords') returns Spark NLP model stopwords_gl\n","For language <el> NLU provides the following Models : \n","nlu.load('el.stopwords') returns Spark NLP model stopwords_el\n","For language <ha> NLU provides the following Models : \n","nlu.load('ha.stopwords') returns Spark NLP model stopwords_ha\n","For language <he> NLU provides the following Models : \n","nlu.load('he.stopwords') returns Spark NLP model stopwords_he\n","For language <hi> NLU provides the following Models : \n","nlu.load('hi.stopwords') returns Spark NLP model stopwords_hi\n","For language <hu> NLU provides the following Models : \n","nlu.load('hu.stopwords') returns Spark NLP model stopwords_hu\n","For language <id> NLU provides the following Models : \n","nlu.load('id.stopwords') returns Spark NLP model stopwords_id\n","For language <ga> NLU provides the following Models : \n","nlu.load('ga.stopwords') returns Spark NLP model stopwords_ga\n","For language <ja> NLU provides the following Models : \n","nlu.load('ja.stopwords') returns Spark NLP model stopwords_ja\n","For language <la> NLU provides the following Models : \n","nlu.load('la.stopwords') returns Spark NLP model stopwords_la\n","For language <lv> NLU provides the following Models : \n","nlu.load('lv.stopwords') returns Spark NLP model stopwords_lv\n","For language <mr> NLU provides the following Models : \n","nlu.load('mr.stopwords') returns Spark NLP model stopwords_mr\n","For language <fa> NLU provides the following Models : \n","nlu.load('fa.stopwords') returns Spark NLP model stopwords_fa\n","For language <ro> NLU provides the following Models : \n","nlu.load('ro.stopwords') returns Spark NLP model stopwords_ro\n","For language <sk> NLU provides the following Models : \n","nlu.load('sk.stopwords') returns Spark NLP model stopwords_sk\n","For language <sl> NLU provides the following Models : \n","nlu.load('sl.stopwords') returns Spark NLP model stopwords_sl\n","For language <so> NLU provides the following Models : \n","nlu.load('so.stopwords') returns Spark NLP model stopwords_so\n","For language <st> NLU provides the following Models : \n","nlu.load('st.stopwords') returns Spark NLP model stopwords_st\n","For language <sw> NLU provides the following Models : \n","nlu.load('sw.stopwords') returns Spark NLP model stopwords_sw\n","For language <sv> NLU provides the following Models : \n","nlu.load('sv.stopwords') returns Spark NLP model stopwords_sv\n","For language <th> NLU provides the following Models : \n","nlu.load('th.stopwords') returns Spark NLP model stopwords_th\n","For language <tr> NLU provides the following Models : \n","nlu.load('tr.stopwords') returns Spark NLP model stopwords_tr\n","For language <yo> NLU provides the following Models : \n","nlu.load('yo.stopwords') returns Spark NLP model stopwords_yo\n","For language <zu> NLU provides the following Models : \n","nlu.load('zu.stopwords') returns Spark NLP model stopwords_zu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TsRxB950elTp"},"source":["## 4.1 Let's try German stopword removal!"]},{"cell_type":"code","metadata":{"id":"5d_J7-20dvCw","executionInfo":{"status":"ok","timestamp":1603729300695,"user_tz":-300,"elapsed":107654,"user":{"displayName":"Gammer Otaku","photoUrl":"","userId":"18042713576744284398"}},"outputId":"44dd6f59-02d7-4ffc-d7b2-0902834fafe1","colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["nlu.load('stopwords').predict(\"Er war von der Vielfältigkeit des NLU Packets begeistert\",output_level='token')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["stopwords_en download started this may take some time.\n","Approximate size to download 2.9 KB\n","[OK!]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cleanTokens</th>\n","      <th>token</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Er, war, von, der, Vielfältigkeit, des, NLU, ...</td>\n","      <td>Er</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[Er, war, von, der, Vielfältigkeit, des, NLU, ...</td>\n","      <td>war</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[Er, war, von, der, Vielfältigkeit, des, NLU, ...</td>\n","      <td>von</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[Er, war, von, der, Vielfältigkeit, des, NLU, ...</td>\n","      <td>der</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[Er, war, von, der, Vielfältigkeit, des, NLU, ...</td>\n","      <td>Vielfältigkeit</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[Er, war, von, der, Vielfältigkeit, des, NLU, ...</td>\n","      <td>des</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[Er, war, von, der, Vielfältigkeit, des, NLU, ...</td>\n","      <td>NLU</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[Er, war, von, der, Vielfältigkeit, des, NLU, ...</td>\n","      <td>Packets</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[Er, war, von, der, Vielfältigkeit, des, NLU, ...</td>\n","      <td>begeistert</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                    cleanTokens           token\n","origin_index                                                                   \n","0             [Er, war, von, der, Vielfältigkeit, des, NLU, ...              Er\n","0             [Er, war, von, der, Vielfältigkeit, des, NLU, ...             war\n","0             [Er, war, von, der, Vielfältigkeit, des, NLU, ...             von\n","0             [Er, war, von, der, Vielfältigkeit, des, NLU, ...             der\n","0             [Er, war, von, der, Vielfältigkeit, des, NLU, ...  Vielfältigkeit\n","0             [Er, war, von, der, Vielfältigkeit, des, NLU, ...             des\n","0             [Er, war, von, der, Vielfältigkeit, des, NLU, ...             NLU\n","0             [Er, war, von, der, Vielfältigkeit, des, NLU, ...         Packets\n","0             [Er, war, von, der, Vielfältigkeit, des, NLU, ...      begeistert"]},"metadata":{"tags":[]},"execution_count":4}]}]}