{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_ELECTRA_Sentence_Embeddings_and_t-SNE_visualization_Example.ipynb","provenance":[{"file_id":"1pgqoRJ6yGWbTLWdLnRvwG5DLSU3rxuMq","timestamp":1599401652794},{"file_id":"1JrlfuV2jNGTdOXvaWIoHTSf6BscDMkN7","timestamp":1599401257319},{"file_id":"1svpqtC3cY6JnRGeJngIPl2raqxdowpyi","timestamp":1599400881246},{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rBXrqlGEYA8G"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/collab/Embeddings_for_Sentences/NLU_ELECTRA_Sentence_Embeddings_and_t-SNE_visualization_Example.ipynb)\n","\n","# ELECTRA Sentence Embeddings with NLU \n","\n","A text encoder trained to distinguish real input tokens from plausible fakes efficiently learns effective language representations.\n","\n","### Sources :\n","- https://arxiv.org/abs/2003.10555\n","\n","### Paper abstract :\n","\n","Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.\n","\n","\n","# 1. Install Java and NLU"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ"},"source":["\n","import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! pip install nlu > /dev/null    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_CL8HZ8Ydry"},"source":["## 2. Load  Model and embed sample sentence with ELECTRA Sentence Embedder"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx"},"source":["import nlu\n","pipe = nlu.load('embed_sentence.electra')\n","pipe.predict('He was suprised by the diversity of NLU')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BAUFklCqLr3V"},"source":["# 3. Download Sample dataset"]},{"cell_type":"code","metadata":{"id":"wAFAOUSuLqvn"},"source":["import pandas as pd\n","# Download the dataset \n","! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv -P /tmp\n","# Load dataset to Pandas\n","df = pd.read_csv('/tmp/train-balanced-sarcasm.csv')\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OPdBQnV46or5"},"source":["# 4. Visualize Embeddings with T-SNE\n","\n","\n","\n","\n","Lets add Sentiment and Part Of Speech to our pipeline because its so easy and so we can hue our T-SNE plots by POS and Sentiment       \n"]},{"cell_type":"code","metadata":{"id":"9bujAZtOCfRW"},"source":["pipe = nlu.load('pos sentiment embed_sentence.electra') # emotion\n","df['text'] = df['comment']\n","\n","# We must set output level to sentence since NLU will infer a different output level for this pipeline composition\n","predictions = pipe.predict(df[['text','label']].iloc[0:10000], output_level='sentence')\n","predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OypFES-8EwY"},"source":["## 4.1 Checkout sentiment distribution"]},{"cell_type":"code","metadata":{"id":"ggbC0PxHgc2t"},"source":["# Some Tokens are None which we must drop first\n","predictions.dropna(how='any', inplace=True)\n","# Some sentiment are 'na' which we must drop first\n","predictions = predictions[predictions.sentiment!= 'na']\n","predictions.sentiment.value_counts().plot.bar(title='Dataset sentiment distribution')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZUYHpsHTINsF"},"source":["# 5.Prepare data for T-SNE algorithm.\n","We create a Matrix with one row per Embedding vector for T-SNE algorithm"]},{"cell_type":"code","metadata":{"id":"L_0jefTB6i52"},"source":["import numpy as np\n","\n","\n","# Make a matrix from the vectors in the np_array column via list comprehension\n","mat = np.matrix([x for x in predictions.embed_sentence_electra_embeddings])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbdi4CY2Iqc0"},"source":["## 5.1 Fit and transform T-SNE algorithm\n"]},{"cell_type":"code","metadata":{"id":"fAFGB6iYIqmO"},"source":["\n","from sklearn.manifold import TSNE\n","model = TSNE(n_components=2) #n_components means the lower dimension\n","low_dim_data = model.fit_transform(mat)\n","print('Lower dim data has shape',low_dim_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gsi0b0XhImaz"},"source":["### Set plotting styles"]},{"cell_type":"code","metadata":{"id":"CsPVw7NHfEgt"},"source":["# set some styles for for Plotting\n","import seaborn as sns\n","# Style Plots a bit\n","sns.set_style('darkgrid')\n","sns.set_palette('muted')\n","sns.set_context(\"notebook\", font_scale=1,rc={\"lines.linewidth\": 2.5})\n","\n","%matplotlib inline\n","import matplotlib as plt\n","plt.rcParams['figure.figsize'] = (20, 14)\n","import matplotlib.pyplot as plt1\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8tuoCxNPmzbo"},"source":["##5.2 Plot low dimensional T-SNE ELECTRA Sentence embeddings with hue for Sarcasm\n"]},{"cell_type":"code","metadata":{"id":"Fbq5MAv0jkft"},"source":["tsne_df =  pd.DataFrame(low_dim_data, predictions.label.replace({1:'sarcasm',0:'normal'}))\n","tsne_df.columns = ['x','y']\n","ax = sns.scatterplot(data=tsne_df, x='x', y='y', hue=tsne_df.index)\n","ax.set_title('T-SNE ELECTRA Sentence Embeddings, colored by Sarcasm label')\n","plt1.savefig(\"electra_sarcasm\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Snb1gtqrnIJi"},"source":["## 5.3 Plot low dimensional T-SNE ELECTRA Sentence embeddings with hue for Sentiment\n"]},{"cell_type":"code","metadata":{"id":"QET-Y6PdnIJt"},"source":["tsne_df =  pd.DataFrame(low_dim_data, predictions.sentiment)\n","tsne_df.columns = ['x','y']\n","ax = sns.scatterplot(data=tsne_df, x='x', y='y', hue=tsne_df.index)\n","ax.set_title('T-SNE ELECTRA Sentence Embeddings, colored by Sentiment')\n","plt1.savefig(\"electra_entiment\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l3sRcFW9muEZ"},"source":["# 6.1 Plot low dimensional T-SNE USE embeddings with hue for POS     \n","Because we will have a list of pos labels for each sentence, we need to explode on the pos column and then do the data peperation for T-SNE again before we can visualize with hue for POS\n"]},{"cell_type":"code","metadata":{"id":"OZ_2DTk9bC-O"},"source":["predictions_exploded_on_pos = predictions.explode('pos')\n","predictions_exploded_on_pos"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k1M_a4pmfMGA"},"source":["## 6.2 Preprocess data for TSNE again"]},{"cell_type":"code","metadata":{"id":"K0rpmiy6a2UK"},"source":["# Make a matrix from the vectors in the np_array column via list comprehension\n","mat = np.matrix([x for x in predictions_exploded_on_pos.embed_sentence_electra_embeddings])\n","\n","\n","from sklearn.manifold import TSNE\n","model = TSNE(n_components=2) #n_components means the lower dimension\n","low_dim_data = model.fit_transform(mat)\n","print('Lower dim data has shape',low_dim_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ze0HWqqfQDh"},"source":["# 6.3 Plot low dimensional T-SNE ELECTRA Sentence embeddings with hue for POS      \n"]},{"cell_type":"code","metadata":{"id":"RB1qdDP3fJHN"},"source":["tsne_df =  pd.DataFrame(low_dim_data, predictions_exploded_on_pos.pos)\n","tsne_df.columns = ['x','y']\n","ax = sns.scatterplot(data=tsne_df, x='x', y='y', hue=tsne_df.index)\n","ax.set_title('T-SNE ELECTRA Sentence Embeddings, colored by Part of Speech Tag')\n","plt1.savefig(\"electra_pos\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uXb-FMA6mX13"},"source":["# 7. NLU has many more sentence embedding models!      \n","Make sure to try them all out!       \n","You can change 'embed_sentence.electra' in nlu.load('embed_sentence.electra') to bert, xlnet, albert or any other of the **20+ sentence embeddings** offerd by NLU"]},{"cell_type":"code","metadata":{"id":"9qUF7jPlme-R"},"source":["nlu.print_all_model_kinds_for_action('embed_sentence')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MvSC3rl5-adJ"},"source":[""],"execution_count":null,"outputs":[]}]}