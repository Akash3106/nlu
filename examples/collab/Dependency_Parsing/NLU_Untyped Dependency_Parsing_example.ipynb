{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_Untyped Dependency_Parsing_example.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"s4ljYpQNp50r"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/collab/Dependency_Parsing/NLU_Untyped%20Dependency_Parsing_example.ipynb)\n","\n","\n","# Untyped Dependency Parsing with NLU. \n","![](https://nlp.johnsnowlabs.com/assets/images/dependency_parser.png)\n","\n","Each word in a sentence has a grammatical relation to other words in the sentence.     \n","These relation pairs can be typed (i.e. subject or pronouns)     or they can be untyped, in which case only the edges between the tokens will be predicted, withouth the label.\n","\n","With NLU you can get these relations in just 1 line of code! \n","# 1. Install Java and NLU"]},{"cell_type":"code","metadata":{"id":"SF5-Z-U4jukd"},"source":["import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","# ! pip install nlu > /dev/null   \n","! pip install nlu > /dev/null    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kHtLKNXDtZf5"},"source":["# 2. Load the Dependency model and predict some sample relationships"]},{"cell_type":"code","metadata":{"id":"7GJX5d6mjk5j","executionInfo":{"status":"ok","timestamp":1603724207402,"user_tz":-300,"elapsed":137956,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"548ccd2e-77d9-4c47-a2ec-ce55c528f844","colab":{"base_uri":"https://localhost:8080/","height":512}},"source":["import nlu\n","dependency_pipe  = nlu.load('dep.untyped')\n","dependency_pipe.predict('Untyped dependencies describe with their relationship a directed graph')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dependency_typed_conllu download started this may take some time.\n","Approximate size to download 257.4 KB\n","[OK!]\n","dependency_conllu download started this may take some time.\n","Approximate size to download 16.6 MB\n","[OK!]\n","pos_anc download started this may take some time.\n","Approximate size to download 4.3 MB\n","[OK!]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>labled_dependency</th>\n","      <th>dependency</th>\n","      <th>pos</th>\n","      <th>token</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>root</td>\n","      <td>ROOT</td>\n","      <td>NNP</td>\n","      <td>Untyped</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>nsubj</td>\n","      <td>describe</td>\n","      <td>NNS</td>\n","      <td>dependencies</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>parataxis</td>\n","      <td>Untyped</td>\n","      <td>VBP</td>\n","      <td>describe</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>det</td>\n","      <td>relationship</td>\n","      <td>IN</td>\n","      <td>with</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>appos</td>\n","      <td>relationship</td>\n","      <td>PRP$</td>\n","      <td>their</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>nsubj</td>\n","      <td>describe</td>\n","      <td>NN</td>\n","      <td>relationship</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>nsubj</td>\n","      <td>graph</td>\n","      <td>DT</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>amod</td>\n","      <td>graph</td>\n","      <td>JJ</td>\n","      <td>directed</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>flat</td>\n","      <td>relationship</td>\n","      <td>NN</td>\n","      <td>graph</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             labled_dependency    dependency   pos         token\n","origin_index                                                    \n","0                         root          ROOT   NNP       Untyped\n","0                        nsubj      describe   NNS  dependencies\n","0                    parataxis       Untyped   VBP      describe\n","0                          det  relationship    IN          with\n","0                        appos  relationship  PRP$         their\n","0                        nsubj      describe    NN  relationship\n","0                        nsubj         graph    DT             a\n","0                         amod         graph    JJ      directed\n","0                         flat  relationship    NN         graph"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"5lrDNzw3tcqT"},"source":["# 3.1 Download sample dataset"]},{"cell_type":"code","metadata":{"id":"gpeS8DWBlrun","executionInfo":{"status":"ok","timestamp":1603724218929,"user_tz":-300,"elapsed":149476,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"4b739744-68d2-4ff1-efc1-2b22cd830ea2","colab":{"base_uri":"https://localhost:8080/","height":777}},"source":["import pandas as pd\n","# Download the dataset \n","! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv -P /tmp\n","# Load dataset to Pandas\n","df = pd.read_csv('/tmp/train-balanced-sarcasm.csv')\n","df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-10-26 14:56:46--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.176.141\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.176.141|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 255268960 (243M) [text/csv]\n","Saving to: ‘/tmp/train-balanced-sarcasm.csv’\n","\n","train-balanced-sarc 100%[===================>] 243.44M  45.0MB/s    in 5.8s    \n","\n","2020-10-26 14:56:52 (41.6 MB/s) - ‘/tmp/train-balanced-sarcasm.csv’ saved [255268960/255268960]\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>comment</th>\n","      <th>author</th>\n","      <th>subreddit</th>\n","      <th>score</th>\n","      <th>ups</th>\n","      <th>downs</th>\n","      <th>date</th>\n","      <th>created_utc</th>\n","      <th>parent_comment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>NC and NH.</td>\n","      <td>Trumpbart</td>\n","      <td>politics</td>\n","      <td>2</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-10</td>\n","      <td>2016-10-16 23:55:23</td>\n","      <td>Yeah, I get that argument. At this point, I'd ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>You do know west teams play against west teams...</td>\n","      <td>Shbshb906</td>\n","      <td>nba</td>\n","      <td>-4</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-11</td>\n","      <td>2016-11-01 00:24:10</td>\n","      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>They were underdogs earlier today, but since G...</td>\n","      <td>Creepeth</td>\n","      <td>nfl</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2016-09</td>\n","      <td>2016-09-22 21:45:37</td>\n","      <td>They're favored to win.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>This meme isn't funny none of the \"new york ni...</td>\n","      <td>icebrotha</td>\n","      <td>BlackPeopleTwitter</td>\n","      <td>-8</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-10</td>\n","      <td>2016-10-18 21:03:47</td>\n","      <td>deadass don't kill my buzz</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>I could use one of those tools.</td>\n","      <td>cush2push</td>\n","      <td>MaddenUltimateTeam</td>\n","      <td>6</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-12</td>\n","      <td>2016-12-30 17:00:13</td>\n","      <td>Yep can confirm I saw the tool they use for th...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1010821</th>\n","      <td>1</td>\n","      <td>I'm sure that Iran and N. Korea have the techn...</td>\n","      <td>TwarkMain</td>\n","      <td>reddit.com</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2009-04</td>\n","      <td>2009-04-25 00:47:52</td>\n","      <td>No one is calling this an engineered pathogen,...</td>\n","    </tr>\n","    <tr>\n","      <th>1010822</th>\n","      <td>1</td>\n","      <td>whatever you do, don't vote green!</td>\n","      <td>BCHarvey</td>\n","      <td>climate</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2009-05</td>\n","      <td>2009-05-14 22:27:40</td>\n","      <td>In a move typical of their recent do-nothing a...</td>\n","    </tr>\n","    <tr>\n","      <th>1010823</th>\n","      <td>1</td>\n","      <td>Perhaps this is an atheist conspiracy to make ...</td>\n","      <td>rebelcommander</td>\n","      <td>atheism</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2009-01</td>\n","      <td>2009-01-11 00:22:57</td>\n","      <td>Screw the Disabled--I've got to get to Church ...</td>\n","    </tr>\n","    <tr>\n","      <th>1010824</th>\n","      <td>1</td>\n","      <td>The Slavs got their own country - it is called...</td>\n","      <td>catsi</td>\n","      <td>worldnews</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2009-01</td>\n","      <td>2009-01-23 21:12:49</td>\n","      <td>I've always been unsettled by that. I hear a l...</td>\n","    </tr>\n","    <tr>\n","      <th>1010825</th>\n","      <td>1</td>\n","      <td>values, as in capitalism .. there is good mone...</td>\n","      <td>frogking</td>\n","      <td>politics</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2009-01</td>\n","      <td>2009-01-24 06:20:14</td>\n","      <td>Why do the people who make our laws seem unabl...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1010826 rows × 10 columns</p>\n","</div>"],"text/plain":["         label  ...                                     parent_comment\n","0            0  ...  Yeah, I get that argument. At this point, I'd ...\n","1            0  ...  The blazers and Mavericks (The wests 5 and 6 s...\n","2            0  ...                            They're favored to win.\n","3            0  ...                         deadass don't kill my buzz\n","4            0  ...  Yep can confirm I saw the tool they use for th...\n","...        ...  ...                                                ...\n","1010821      1  ...  No one is calling this an engineered pathogen,...\n","1010822      1  ...  In a move typical of their recent do-nothing a...\n","1010823      1  ...  Screw the Disabled--I've got to get to Church ...\n","1010824      1  ...  I've always been unsettled by that. I hear a l...\n","1010825      1  ...  Why do the people who make our laws seem unabl...\n","\n","[1010826 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"uLWu8DG3tfjz"},"source":["## 3.2 Predict on sample dataset\n","NLU expects a text column, thus we must create it from the column that contains our text data"]},{"cell_type":"code","metadata":{"id":"3V5l-B6nl43U","executionInfo":{"status":"ok","timestamp":1603724247131,"user_tz":-300,"elapsed":177669,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"557a1854-3504-4155-e469-081c2ea8b63f","colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["dependency_pipe  = nlu.load('dep.untyped')\n","df['text'] = df['comment']\n","dependency_predictions = dependency_pipe.predict(df['text'].iloc[0:500])\n","dependency_predictions"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dependency_typed_conllu download started this may take some time.\n","Approximate size to download 257.4 KB\n","[OK!]\n","dependency_conllu download started this may take some time.\n","Approximate size to download 16.6 MB\n","[OK!]\n","pos_anc download started this may take some time.\n","Approximate size to download 4.3 MB\n","[OK!]\n","INFO: NLU will assume text as label column since default text column could not be find\n","No accepted Data type or usable columns found or applying the NLU models failed. \n","Make sure that the first column you pass to .predict() is the one that nlu should predict on OR rename the column you want to predict on to 'text'  \n","If you are on Google Collab, click on Run time and try factory reset Runtime run the setup script again, you might have used too much memory\n","On Kaggle try to reset restart session and run the setup script again, you might have used too much memory\n","Full Stacktrace was (<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o1568.collectToPython.\\n', JavaObject id=o1575), <traceback object at 0x7f9f8f668408>)\n","Additional info:\n","<class 'py4j.protocol.Py4JJavaError'> pipeline.py 1087\n","Stuck? Contact us on Slack! https://join.slack.com/t/spark-nlp/shared_invite/zt-7rd4kw03-7F44zohcrUo0RULCd8rYrw\n"],"name":"stdout"}]}]}